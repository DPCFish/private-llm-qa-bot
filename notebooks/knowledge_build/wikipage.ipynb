{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Seleium to crawl website directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from typing import Any, Dict, List, Optional\n",
    "from pydantic import BaseModel,Extra\n",
    "from datetime import datetime\n",
    "from selenium.common import exceptions  \n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1067,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using googal Chrome\n",
    "options = Options()\n",
    "\n",
    "# options.add_argument(\"--headless=new\") #running in headless mode\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "#Using firefox \n",
    "# options = webdriver.FirefoxOptions()\n",
    "# driver = webdriver.Firefox(options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os, json\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class CrawlerWorkder(BaseModel):\n",
    "    url_map:Dict[str,int]   ## {'url':depth}\n",
    "    url_processed:Dict[str,int] = {}\n",
    "    driver:Any\n",
    "    max_depth:int = 1 \n",
    "    content:List[Any] = []\n",
    "    save_path:str = 'temp_webcontent/'\n",
    "    prefix:str = None\n",
    "    body_id:str\n",
    "    extract_wiki_href:bool = False\n",
    "    offset:int = 10\n",
    "    url_validate_callback: Optional[Any] = None\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "        extra = Extra.forbid\n",
    "\n",
    "\n",
    "    def _savefile(self):\n",
    "        os.makedirs(os.path.dirname(f'{self.save_path}{self.prefix}/'), exist_ok=True)\n",
    "        str_date = datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\") \n",
    "        with open(f'{self.save_path}{self.prefix}/file-{str_date}.wiki','w') as f:\n",
    "            f.write(json.dumps(self.content,ensure_ascii=False))\n",
    "\n",
    "\n",
    "    ## open wiki page in source code mode, to parse hrefs\n",
    "    def _find_hrefs(self,url:str) -> List[Any]:\n",
    "        suffix = '/WebHome?viewer=code'\n",
    "        driver.get(url+suffix)\n",
    "        current_hrefs = []\n",
    "        try:\n",
    "            main_container = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.ID, self.body_id)))\n",
    "        except exceptions.TimeoutException as e:\n",
    "            print (f\"{url} error: timeout, please check the div ID, or network connection\")\n",
    "            return current_hrefs\n",
    "        if not main_container:\n",
    "            return current_hrefs\n",
    "        text = main_container.text\n",
    "        it = re.finditer(r'\\[\\[([^\\]]+)>>((url:)?https?:\\/\\/[^\\]]+)\\]\\]',text,re.M)\n",
    "        p = re.compile(r'(\\s)+')\n",
    "        for match in it: \n",
    "            a,b = match.span()\n",
    "            offset_index = 0 if a - self.offset < 0 else a - self.offset\n",
    "            # print(text[offset_index:a])\n",
    "            key = re.sub(r'(.*)]]','',text[offset_index:a])\n",
    "\n",
    "            # excluded case such as: [[ >>https://ec2-maitre-d-prod-iad.iad.proxy.amazon.com/]]\n",
    "            if not p.match(match.group(1)):\n",
    "                current_hrefs.append([key,match.group(1),match.group(2)])\n",
    "        return current_hrefs\n",
    "        \n",
    "\n",
    "    def _call(self,url:str,current_depth:int) ->List [List[Any],Dict[str,int]]:\n",
    "        \"\"\"crawl website infor\"\"\"\n",
    "        # text = ['Purpose','Region Build Automation Framework','1 - Complete Prerequisites Guide']\n",
    "        # url_map = {'https://w.amazon.com/bin/view/AWSRegionBuildEngineering/RIP/FAQ':current_depth+1}\n",
    "        text = []\n",
    "        current_url_map = {}\n",
    "        print(f'processing:{url}   depth:{current_depth}')\n",
    "        self.url_processed[url] = current_depth\n",
    "        driver.get(url)\n",
    "        # driver.implicitly_wait(1)\n",
    "        try:\n",
    "            main_container = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.ID, self.body_id)))\n",
    "        except exceptions.TimeoutException as e:\n",
    "            print (f\"{url} error: timeout, please check the div ID, or network connection\")\n",
    "            return text,current_url_map\n",
    "        # main_container = driver.find_element(By.ID, self.body_id)\n",
    "        if not main_container:\n",
    "            return text,current_url_map\n",
    "        origintext = main_container.text\n",
    "        ##获取下一级的链接\n",
    "        links = main_container.find_elements(By.TAG_NAME,'a')\n",
    "        for link in links:\n",
    "            ## 跳过空的链接\n",
    "            try: \n",
    "                if not link.text:\n",
    "                    continue\n",
    "            except exceptions.StaleElementReferenceException as e:\n",
    "                print(e)\n",
    "                continue\n",
    "            href = link.get_attribute('href')\n",
    "            if not href:\n",
    "                continue\n",
    "            ##检查url\n",
    "            if self.url_validate_callback:\n",
    "                if not self.url_validate_callback(href):\n",
    "                    continue\n",
    "            current_url_map[href]=current_depth+1\n",
    "         \n",
    "        ## 提取wiki中的链接，转成md格式\n",
    "        if self.extract_wiki_href:\n",
    "            current_hrefs = self._find_hrefs(url)\n",
    "            for a,b,c in current_hrefs:\n",
    "                c = c.replace('url:','')\n",
    "                if origintext.find(f'{a}{b}'):\n",
    "                    origintext = origintext.replace(f'{a}{b}',f'{a}[{b}]({c})',1)\n",
    "                    print (f'{a}{b} ==> {a}[{b}]({c})')\n",
    "        text.append(origintext) \n",
    "        \n",
    "        return text,current_url_map\n",
    "\n",
    "    def destroy(self):\n",
    "        print('quite driver')\n",
    "        self.driver.quit()\n",
    "\n",
    "    def start(self) ->List[Any]:\n",
    "        self.prefix = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\") \n",
    "        while 1:\n",
    "            if len(self.url_map) == 0:\n",
    "                break\n",
    "            url, depth = self.url_map.popitem()\n",
    "            url = url.rstrip('/')\n",
    "            if url in self.url_processed:\n",
    "                continue\n",
    "            # print(len(self.url_map))\n",
    "            ##如果不超过最大depth则抓取\n",
    "            if depth <= self.max_depth: \n",
    "                text,new_url_map = self._call(url,depth)\n",
    "                if len(text):\n",
    "                    self.content.append({url:text})\n",
    "                #检查url是否重复\n",
    "                if url in new_url_map:\n",
    "                    del new_url_map[url]\n",
    "                #合并新的\n",
    "                self.url_map = {**self.url_map,**new_url_map}\n",
    "            self._savefile()\n",
    "        print(f'processed urls:{self.url_processed}')\n",
    "        return self.content\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1069,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate_wiki_url(url):\n",
    "    # pattern = re.compile(r'^https?://(?:www\\.)?[a-zA-Z0-9-]+\\.[a-zA-Z]{2,}(?:/[^\\s]*)?$')\n",
    "    pattern = re.compile(r'^https://w(iki)?.amazon.com/bin/view(?:/[^\\s#]*)?$')\n",
    "    matched  = re.match(pattern, url) \n",
    "    if not matched:\n",
    "        return False \n",
    "    ## exclude ticket\n",
    "    elif url.startswith('https://t.corp.amazon.com'):\n",
    "        return False\n",
    "    elif url.find('WebHome') > -1:\n",
    "        return False\n",
    "    elif re.match(r'https://w(iki)?.amazon.com/bin/view/Main',url,re.I):\n",
    "        return False\n",
    "    elif re.match(r'https://w(iki)?.amazon.com/bin/view/KnowledgeTech',url,re.I):\n",
    "        return False\n",
    "    elif re.match(r'https://w(iki)?.amazon.com/bin/view/WikiManager',url,re.I): \n",
    "        return False\n",
    "    elif re.match(r'https://w(iki)?.amazon.com/bin/view/AmazonWiki/Wiki/Help',url,re.I): \n",
    "        return False\n",
    "    elif re.match(r'https://w(iki)?.amazon.com/bin/view/XWiki',url,re.I): \n",
    "        return False\n",
    "    elif re.match(r'https://w(iki)?.amazon.com/bin/view/Users/',url,re.I): \n",
    "        return False\n",
    "    elif re.match(r'https://w(iki)?.amazon.com/bin/view/Bindles/',url,re.I): \n",
    "        return False\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(validate_url('https://w.amazon.com/bin/create/AWS_GCR_GTMS/ServiceLaunch/FOOBguidance'))\n",
    "# print(validate_url('https://w.amazon.com/bin/view/RegionBuildAutomation/FAQ'))\n",
    "# print(validate_url('https://wiki.amazon.com/bin/viewrev/RegionBuildAutomation/FAQ/WebHome?viewer=code&rev=33.1'))\n",
    "# print(validate_url('https://w.amazon.com/bin/view/AWS_GCR_GTMS/ServiceLaunch/FOOBguidance'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_url = {'https://w.amazon.com/bin/view/AWS_GCR_GTMS/ServiceLaunch/FOOBguidance':0}\n",
    "# input_url = {'https://wiki.amazon.com/bin/view/RegionBuildAutomation/FAQ':0}\n",
    "# input_url = {'https://w.amazon.com/bin/view/EC2_Capacity_Planning_-_External_Capacity_Runbook':0}\n",
    "\n",
    "cwoker = CrawlerWorkder(url_map=input_url,\n",
    "                        driver=driver,\n",
    "                        max_depth=1,\n",
    "                        url_validate_callback=validate_wiki_url,\n",
    "                        body_id='mainContentArea',\n",
    "                        extract_wiki_href=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:https://w.amazon.com/bin/view/AWS_GCR_GTMS/ServiceLaunch/FOOBguidance   depth:0\n",
      " via this link ==>  via this [link](https://ec2-maitre-d-prod-iad.iad.proxy.amazon.com/)\n",
      "your team here  ==> your team [here ](https://permissions.amazon.com/a/team/Maitre%20D%20-%20Users)\n",
      "' section here ==> ' section [here](https://w.amazon.com/bin/view/EC2/DemandShaping/MaitreD/UserGuide/)\n",
      "ser guide here ==> ser guide [here](https://w.amazon.com/bin/view/EC2/DemandShaping/MaitreD/UserGuide/)\n",
      "equest in MaitreD’ ==> equest in [MaitreD’](https://ec2-maitre-d-prod-iad.iad.proxy.amazon.com/batchCreate?noTrigger=true)\n",
      "processing:https://w.amazon.com/bin/view/EC2_Capacity_Planning_-_External_Capacity_Runbook   depth:1\n",
      " via this link ==>  via this [link](https://ec2-maitre-d-prod-iad.iad.proxy.amazon.com/)\n",
      "your team here  ==> your team [here ](https://permissions.amazon.com/a/team/Maitre%20D%20-%20Users)\n",
      "' section here ==> ' section [here](https://w.amazon.com/bin/view/EC2/DemandShaping/MaitreD/UserGuide/)\n",
      "ser guide here ==> ser guide [here](https://w.amazon.com/bin/view/EC2/DemandShaping/MaitreD/UserGuide/)\n",
      "equest in MaitreD’ ==> equest in [MaitreD’](https://ec2-maitre-d-prod-iad.iad.proxy.amazon.com/batchCreate?noTrigger=true)\n",
      "processed urls:{'https://w.amazon.com/bin/view/AWS_GCR_GTMS/ServiceLaunch/FOOBguidance': 0, 'https://w.amazon.com/bin/view/EC2_Capacity_Planning_-_External_Capacity_Runbook': 1}\n"
     ]
    }
   ],
   "source": [
    "content = cwoker.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1065,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 估算文档切分token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "def num_tokens_from_string(string: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    # encoding = tiktoken.get_encoding(encoding_name)\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = '/Users/chuanxie/Downloads/datalake/web/myapp/private-llm-qa-bot/notebooks/knowledge_build/temp_webcontent/2023-08-24-10-18-43/file-2023-08-24-10:20:06.wiki' ##FOOB Runbook \n",
    "# doc = 'file-2023-08-23-17:17:02.wiki' ## RBA FAQ\n",
    "with open(doc,'r') as f:\n",
    "    content = f.read()\n",
    "content_json = json.loads(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(        \n",
    "        chunk_size = 800,\n",
    "        chunk_overlap  = 200,\n",
    "        length_function = len,\n",
    ")\n",
    "texts = []\n",
    "for page in content_json:\n",
    "    for url,p in page.items():\n",
    "        if len(p):\n",
    "            print(f'page:{url}, size: {len(p[0])}')\n",
    "            chunks = text_splitter.split_text(p[0])\n",
    "            texts+=chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = ''.join(['-']*150)\n",
    "for text in texts:\n",
    "    print(f'--tokens:{num_tokens_from_string(text)}-{line}')\n",
    "    print(f'{text}\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
