{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c261e5f4-17a8-40da-beb9-599f1717e0fe",
   "metadata": {},
   "source": [
    "### 1. 安装HuggingFace 并下载模型到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02785614-9268-41c8-85a5-d579490edbbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install huggingface-hub -Uqq \n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba24701-47db-4107-9a6c-1667038d0054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf ./LLM_qwen15_int4_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e6bd7ee-16a3-4f5a-8857-8bbba83eb9e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "local_model_path = Path(\"./LLM_qwen15_14b_int4_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3469632-4174-4df4-a7d1-ef167561c626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_model_path.mkdir(exist_ok=True)\n",
    "\n",
    "model_name = \"Qwen/Qwen1.5-14B-Chat-GPTQ-Int4\"\n",
    "commit_hash = \"2303ef27e4d8f3bf668c3139d1653e09fa41c83d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e8abc5-a58e-40e2-b1e6-fbf48307c716",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snapshot_download(repo_id=model_name, revision=commit_hash, cache_dir=local_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d666c79-b039-4258-ac3b-46b19e63c3b8",
   "metadata": {},
   "source": [
    "### 2. 把模型拷贝到S3为后续部署做准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9431deb-6359-442d-847b-1563f8dd3854",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40dd8f16-ae7c-48bf-8e52-1a15425fa74d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_code_prefix: aigc-llm-models/Qwen/Qwen1.5-14B-Chat-GPTQ-Int4_deploy_code\n",
      "model_snapshot_path: LLM_qwen15_14b_int4_model/models--Qwen--Qwen1.5-14B-Chat-GPTQ-Int4/snapshots/2303ef27e4d8f3bf668c3139d1653e09fa41c83d\n"
     ]
    }
   ],
   "source": [
    "s3_model_prefix = f\"aigc-llm-models/{model_name}\"  # folder where model checkpoint will go\n",
    "model_snapshot_path = list(local_model_path.glob(\"**/snapshots/*\"))[0]\n",
    "s3_code_prefix = f\"aigc-llm-models/{model_name}_deploy_code\"\n",
    "print(f\"s3_code_prefix: {s3_code_prefix}\")\n",
    "print(f\"model_snapshot_path: {model_snapshot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067292c9-c066-4649-a61f-b460a24da584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive {model_snapshot_path} s3://{bucket}/{s3_model_prefix}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696b70c3-90f1-4175-95bf-568bafbcd383",
   "metadata": {},
   "source": [
    "### 3. 模型部署准备（entrypoint脚本，容器镜像，服务配置）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f7c4277-4480-42c6-aee6-1fbcca94eb82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.26.0-deepspeed0.12.6-cu121\n"
     ]
    }
   ],
   "source": [
    "# 可以从 https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers 中寻找更新版本的 Container\n",
    "\n",
    "#中国区需要替换为下面的image_uri\n",
    "# inference_image_uri = (\n",
    "#     f\"727897471807.dkr.ecr.{region}.amazonaws.com.cn/djl-inference:0.26.0-deepspeed0.12.6-cu121\"\n",
    "# )\n",
    "\n",
    "inference_image_uri = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.26.0-deepspeed0.12.6-cu121\"\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d771bdb-11d2-45d2-9bef-face29221838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_code_dir = s3_code_prefix.split('/')[-1]\n",
    "!mkdir -p {local_code_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4cc43d4a-228d-487d-b378-116cf3d2cb76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Qwen1.5-14B-Chat-GPTQ-Int4_deploy_code/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {local_code_dir}/model.py\n",
    "from djl_python import Input, Output\n",
    "from djl_python.streaming_utils import StreamingUtils\n",
    "from djl_python.properties_manager.hf_properties import HuggingFaceProperties\n",
    "from djl_python.properties_manager.properties import StreamingEnum, is_rolling_batch_enabled, is_streaming_enabled\n",
    "import os\n",
    "\n",
    "import logging\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = None\n",
    "tokenizer = None\n",
    "hf_configs = None \n",
    "\n",
    "def get_model(properties):\n",
    "    tensor_parallel = properties[\"tensor_parallel_degree\"]\n",
    "    model_location = properties['model_dir']\n",
    "    if \"model_id\" in properties:\n",
    "        model_location = properties['model_id']\n",
    "    logging.info(f\"Loading model in {model_location}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_location, use_fast=False)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_location,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype='auto'\n",
    "    ).eval()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def inference(inputs):\n",
    "    try:\n",
    "        input_map = inputs.get_as_json()\n",
    "        logging.info(f\"(lyb)input_map\")\n",
    "        logging.info(input_map)\n",
    "        \n",
    "        input_properties = inputs.get_properties()\n",
    "        logging.info(f\"(lyb)input_properties\")\n",
    "        logging.info(input_properties)\n",
    "        \n",
    "        data = input_map.pop(\"inputs\", '')\n",
    "        messages = input_map.pop(\"messages\", [])\n",
    "        parameters = input_map.pop(\"parameters\", {})\n",
    "        stream = input_map.pop(\"stream\", False)\n",
    "        outputs = Output()\n",
    "        \n",
    "        max_new_tokens = parameters.pop('max_tokens', 512)\n",
    "        parameters['max_new_tokens'] = max_new_tokens\n",
    "        \n",
    "        if is_streaming_enabled(hf_configs.enable_streaming) and stream:\n",
    "            outputs.add_property(\"content-type\", \"application/jsonlines\")\n",
    "            if hf_configs.enable_streaming.value == StreamingEnum.huggingface.value:\n",
    "                outputs.add_stream_content(\n",
    "                    StreamingUtils.use_hf_default_streamer(\n",
    "                        model, tokenizer, data,\n",
    "                        hf_configs.device, **parameters))\n",
    "            else:\n",
    "                stream_generator = StreamingUtils.get_stream_generator(\n",
    "                    \"Accelerate\")\n",
    "                outputs.add_stream_content(\n",
    "                    stream_generator(model, tokenizer, data,\n",
    "                                     hf_configs.device, **parameters))\n",
    "            return outputs\n",
    "\n",
    "\n",
    "        input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "        output_ids = model.generate(input_ids.to('cuda'), **parameters)\n",
    "        response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "        result = {\"outputs\": response}\n",
    "        outputs.add_as_json(result)\n",
    "        \n",
    "        return outputs\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Huggingface inference failed\")\n",
    "        outputs = Output().error(str(e))\n",
    "\n",
    "\n",
    "def handle(inputs: Input) -> None:\n",
    "    global model, tokenizer, hf_configs\n",
    "    logging.info(f\"global model, tokenizer\")\n",
    "    \n",
    "    properties = inputs.get_properties()\n",
    "    \n",
    "    if not hf_configs:\n",
    "        hf_configs = HuggingFaceProperties(**properties)\n",
    "    \n",
    "    if not model:\n",
    "        model, tokenizer = get_model(properties)\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        return None\n",
    "\n",
    "    logging.info(f\"(lyb)inference\")\n",
    "    return inference(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "aad5bdd9-6adf-43dd-8b1a-d79ff09ea13b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "option.s3url ==> s3://sagemaker-us-west-2-106839800180/aigc-llm-models/Qwen/Qwen1.5-14B-Chat-GPTQ-Int4/\n"
     ]
    }
   ],
   "source": [
    "s3_path = f\"s3://{bucket}/{s3_model_prefix}/\"\n",
    "print(f\"option.s3url ==> {s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06d1e60-3914-4059-a08f-05ac26761165",
   "metadata": {},
   "source": [
    "#### Note: option.s3url 需要按照自己的账号进行修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8996fe44-8e70-468b-abc1-38187cb33f4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Qwen1.5-14B-Chat-GPTQ-Int4_deploy_code/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile {local_code_dir}/serving.properties\n",
    "engine=Python\n",
    "option.tensor_parallel_degree=1\n",
    "option.enable_streaming=True\n",
    "option.predict_timeout=240\n",
    "option.trust_remote_code=true\n",
    "option.s3url = S3PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9cdbb78c-f0d7-4c2e-9501-6f73395d92a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sed -i \"s|option.s3url = S3PATH|option.s3url = {s3_path}|\" {local_code_dir}/serving.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feef22a2-27b9-4018-a46b-6a99b532512f",
   "metadata": {},
   "source": [
    "#### 注意: 必须把transformers升级到4.37.0以上，否则会出现  [Issue34](https://github.com/QwenLM/Qwen1.5/issues/34)\n",
    "\n",
    "如果是中国区建议添加国内的pip镜像,如下代码所示\n",
    "```https://github.com/QwenLM/Qwen1.5/issues/34\n",
    "%%writefile {local_code_dir}/requirements.txt\n",
    "-i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "transformers==4.37.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7b7e76c6-6dbc-47fc-9f47-4765c526ab76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Qwen1.5-14B-Chat-GPTQ-Int4_deploy_code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {local_code_dir}/requirements.txt\n",
    "transformers==4.37.0\n",
    "accelerate\n",
    "tiktoken\n",
    "einops\n",
    "scipy\n",
    "transformers_stream_generator==0.0.4\n",
    "peft\n",
    "deepspeed\n",
    "auto-gptq\n",
    "optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199907e8-dde4-43b5-a6f3-82f46a6bf6f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install auto-gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0ae6734a-aacd-410d-818d-0a962697c3c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen1.5-14B-Chat-GPTQ-Int4_deploy_code/\n",
      "Qwen1.5-14B-Chat-GPTQ-Int4_deploy_code/model.py\n",
      "Qwen1.5-14B-Chat-GPTQ-Int4_deploy_code/requirements.txt\n",
      "Qwen1.5-14B-Chat-GPTQ-Int4_deploy_code/serving.properties\n"
     ]
    }
   ],
   "source": [
    "!rm model.tar.gz\n",
    "!cd {local_code_dir} && rm -rf \".ipynb_checkpoints\"\n",
    "!tar czvf model.tar.gz {local_code_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0f77dc76-6d8c-4665-ba88-f03e887c136c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-west-2-106839800180/aigc-llm-models/Qwen/Qwen1.5-14B-Chat-GPTQ-Int4_deploy_code/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5853daa-b8a3-4485-8c0a-64bf83e93a18",
   "metadata": {},
   "source": [
    "### 4. 创建模型 & 创建endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ef974ca1-9638-45a8-9145-ea9d03b2b072",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen15-14B-int4-2024-03-19-05-41-40-301\n",
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.26.0-deepspeed0.12.6-cu121\n",
      "Created Model: arn:aws:sagemaker:us-west-2:106839800180:model/qwen15-14B-int4-2024-03-19-05-41-40-301\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "import boto3\n",
    "\n",
    "model_name = name_from_base(f\"qwen15-14B-int4\") #Note: Need to specify model_name\n",
    "print(model_name)\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": s3_code_artifact\n",
    "    },\n",
    "    \n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "233bb3a4-d737-41ad-8fcc-7082c6278e8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointConfigArn': 'arn:aws:sagemaker:us-west-2:106839800180:endpoint-config/qwen15-14B-int4-2024-03-19-05-41-40-301-config',\n",
       " 'ResponseMetadata': {'RequestId': '616e1d96-a622-4d07-b72a-9d94aac28fc1',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '616e1d96-a622-4d07-b72a-9d94aac28fc1',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '127',\n",
       "   'date': 'Tue, 19 Mar 2024 05:41:41 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "#Note: ml.g4dn.2xlarge 也可以选择\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g5.2xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            # \"VolumeSizeInGB\" : 400,\n",
    "            # \"ModelDataDownloadTimeoutInSeconds\": 2400,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 10*60,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "734a39b0-473e-4421-94c8-74d2b4105038",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Endpoint: arn:aws:sagemaker:us-west-2:106839800180:endpoint/qwen15-14B-int4-2024-03-19-05-41-40-301-endpoint\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1262e826-a810-401d-a5a9-f62febb24e5f",
   "metadata": {},
   "source": [
    "#### 持续检测模型部署进度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "08969928-6b9e-4d9c-a033-a31f5f77bdfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-west-2:106839800180:endpoint/qwen15-14B-int4-2024-03-19-05-41-40-301-endpoint\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d985b427-3959-46f7-9a50-5a2b45e2d513",
   "metadata": {},
   "source": [
    "### 5. 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e56bfdaa-3469-4784-aa8a-e32177cde3f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.57 ms, sys: 0 ns, total: 4.57 ms\n",
      "Wall time: 3.91 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "parameters = {\n",
    "  \"max_tokens\": 1024,\n",
    "  \"temperature\": 0.1,\n",
    "  \"top_p\":0.8\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619db19b-0072-4d25-a4df-5d59c2f6947b",
   "metadata": {},
   "source": [
    "## No stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d577e076-52b2-4257-a447-1d3a5813d7ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "time:14.543756246566772 s\n",
      "{\n",
      "  \"outputs\":\"标题：星际烽火\\n\\n在遥远的未来，人类文明已经扩张到了银河系的边缘。在星域深处，一场名为\\\"暗影战争\\\"的宇宙战争正在悄然酝酿。地球联盟，作为宇宙中的和平守护者，与黑暗势力——暗影帝国的冲突一触即发。\\n\\n地球联盟的旗舰\\\"曙光号\\\"，由天才科学家艾伦驾驶，他的目标是阻止暗影帝国的邪恶计划——吞噬所有星系的生命能源。艾伦的队伍包括勇敢的战士莉娜和智谋过人的战术家罗伯特，他们一同踏上了这场未知的冒险。\\n\\n在一次深入敌后的任务中，他们发现暗影帝国的首领，黑暗领主，正准备启动一个黑洞引擎，将整个星系的生命力吸干。艾伦一行人决定冒险破坏这个装置，他们利用高科技武器和智慧，与黑暗领主展开了一场惊心动魄的太空对决。\\n\\n在最后关头，艾伦利用他的创新思维，设计出了一种可以干扰黑洞引擎的特殊波束。在生死存亡之际，他们成功地发射了波束，黑洞引擎的能量输出被削弱，暗影领主的计划破产。\\n\\n然而，胜利并未结束他们的战斗。艾伦明白，这只是战争的开始，他们必须继续保护宇宙的和平。\\\"曙光号\\\"再次启航，向着下一个战场，他们准备迎接更多的挑战。\\n\\n在浩渺的宇宙中，战争的烽火照亮了黑暗，而地球联盟，就像一颗永不熄灭的星辰，坚守着和平的信念，继续前行。\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompts1 = \"\"\"AWS Clean Rooms 的FAQ文档有提到 Q: 是否发起者和数据贡献者都会被收费？A: 是单方收费，只有查询的接收方会收费。\n",
    "请问AWS Clean Rooms是多方都会收费吗？\n",
    "\"\"\"\n",
    "prompts1 = \"\"\"写一篇500字的科幻小说，背景关于宇宙战争\"\"\"\n",
    "\n",
    "messages = [{\"role\":\"user\", \"content\": prompts1}]\n",
    "\n",
    "start = time.time()\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            Body=json.dumps(\n",
    "            {\n",
    "                \"inputs\" : prompts1,\n",
    "                \"messages\": messages,\n",
    "                \"parameters\": parameters\n",
    "            }\n",
    "            ),\n",
    "            ContentType=\"application/json\",\n",
    "        )\n",
    "\n",
    "resp = response_model['Body'].read()\n",
    "print (f\"\\ntime:{time.time()-start} s\")\n",
    "print(resp.decode('utf8'))\n",
    "# print(resp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92ed208-ee3a-422d-a9f2-06cf3d515cb4",
   "metadata": {},
   "source": [
    "## stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "adf07e63-590c-4885-9f43-76714e1e86e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "\n",
    "class StreamScanner:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the InvokeEndpointWithResponseStream event stream. \n",
    "    \n",
    "    The output of the model will be in the following format:\n",
    "    ```\n",
    "    b'{\"outputs\": [\" a\"]}\\n'\n",
    "    b'{\"outputs\": [\" challenging\"]}\\n'\n",
    "    b'{\"outputs\": [\" problem\"]}\\n'\n",
    "    ...\n",
    "    ```\n",
    "    \n",
    "    While usually each PayloadPart event from the event stream will contain a byte array \n",
    "    with a full json, this is not guaranteed and some of the json objects may be split across\n",
    "    PayloadPart events. For example:\n",
    "    ```\n",
    "    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n",
    "    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n",
    "    ```\n",
    "    \n",
    "    This class accounts for this by concatenating bytes written via the 'write' function\n",
    "    and then exposing a method which will return lines (ending with a '\\n' character) within\n",
    "    the buffer via the 'readlines' function. It maintains the position of the last read \n",
    "    position to ensure that previous bytes are not exposed again. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.buff = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "        \n",
    "    def write(self, content):\n",
    "        self.buff.seek(0, io.SEEK_END)\n",
    "        self.buff.write(content)\n",
    "        \n",
    "    def readlines(self):\n",
    "        self.buff.seek(self.read_pos)\n",
    "        for line in self.buff.readlines():\n",
    "            if line[-1] != b'\\n':\n",
    "                self.read_pos += len(line)\n",
    "                yield line[:-1]\n",
    "                \n",
    "    def reset(self):\n",
    "        self.read_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0d7d14b0-a78d-41a9-b68b-ac9ae79cd729",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "。\n",
      "标题：星际的决战\n",
      "\n",
      "在遥远的未来，人类已经掌握了星际旅行的技术，建立了庞大的星际联盟。然而，宇宙的和平并未长久，一股名为“黑暗星云”的外星势力，他们的目标是吞噬所有的文明，宇宙战争一触即发。\n",
      "\n",
      "我们的主角，年轻的星际战士艾伦，被选中参加这场决定人类命运的战争。他的飞船，名为“希望号”，装备了最先进的防御系统和武器。艾伦的使命是保护联盟的核心星球——地球。\n",
      "\n",
      "战争的初期，黑暗星云的军队如潮水般涌来，艾伦和他的队伍在星空中奋力抵抗。每一次的战斗都像是在生死边缘徘徊，但艾伦从未退缩，他的决心和勇气激励着整个联盟。\n",
      "\n",
      "在一次关键的战役中，艾伦发现黑暗星云的首领——黑暗领主，他的力量源自一颗名为“毁灭之心”的神秘星球。艾伦决定冒险深入黑暗星云的腹地，寻找并摧毁这颗星球。\n",
      "\n",
      "经过一系列的冒险和挑战，艾伦成功摧毁了“毁灭之心”，黑暗星云的军队瞬间瓦解。艾伦的勇敢和智慧，成为了联盟的英雄，他的名字被刻在了宇宙的历史中。\n",
      "\n",
      "宇宙战争的硝烟散去，和平再次降临。艾伦回到地球，看着星空，心中充满了对未来的期待。他知道，虽然战争结束了，但人类的探索和守护宇宙的使命，才刚刚开始。<|endoftext|><|im_start|>\n",
      "<|im_start|><|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>"
     ]
    }
   ],
   "source": [
    "prompts1 = \"\"\"写一篇500字的科幻小说，背景关于宇宙战争\"\"\"\n",
    "\n",
    "messages = [{\"role\":\"user\", \"content\": prompts1}]\n",
    "\n",
    "start = time.time()\n",
    "response_model = smr_client.invoke_endpoint_with_response_stream(\n",
    "            EndpointName=endpoint_name,\n",
    "            Body=json.dumps(\n",
    "            {\n",
    "                \"inputs\" : [prompts1],\n",
    "                \"messages\": messages,\n",
    "                \"parameters\": parameters,\n",
    "                \"stream\" : True\n",
    "            }\n",
    "            ),\n",
    "            ContentType=\"application/json\",\n",
    "        )\n",
    "\n",
    "event_stream = response_model['Body']\n",
    "scanner = StreamScanner()\n",
    "for event in event_stream:\n",
    "    scanner.write(event['PayloadPart']['Bytes'])\n",
    "    for line in scanner.readlines():\n",
    "        try:\n",
    "            resp = json.loads(line.decode('utf-8'))\n",
    "            \n",
    "            print(resp['outputs'][0], end='')\n",
    "        except Exception as e:\n",
    "            print(line)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c8b703-e312-4964-8be9-a754468e07cd",
   "metadata": {},
   "source": [
    "#### 清除模型Endpoint和config(仅限清除资源时使用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f70d116f-4fb1-4f04-8732-3d6e4fb520de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws sagemaker delete-endpoint --endpoint-name {endpoint_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "184e4d1d-3d62-43df-9b17-5d64ece928bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws sagemaker delete-endpoint-config --endpoint-config-name {endpoint_config_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "707e8f09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws sagemaker delete-model --model-name {model_name}"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
