{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73337842-6f48-4df6-a3e1-d071a1864ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/FlagOpen/FlagEmbedding.git -Uq\n",
    "!pip install -Uq sentence-transformers \n",
    "!pip install faiss-cpu -Uq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cd5298-0542-4e1a-8d45-1b9b561edf05",
   "metadata": {},
   "source": [
    "### Generate random negative label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a3dd10b-b601-4165-b902-f2e4ac5baa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use chatgpt synthesis data\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "FAQ_FILE = 'chatgpt_synthesis.jsonl'\n",
    "\n",
    "def generate_train_data(train_data, output_suffix='train'):\n",
    "    size_train_data = len(train_data)\n",
    "    qq_labels = []\n",
    "    oqgd_labels = []\n",
    "    gqod_labels = []\n",
    "    for line in train_data:\n",
    "        json_obj = json.loads(line)\n",
    "        \n",
    "        o_query = json_obj['origin_question'].strip()\n",
    "        o_answer = json_obj['origin_answer']\n",
    "        g_query = json_obj['generate_question']\n",
    "        g_answer = json_obj['generate_answer']\n",
    "        o_doc = f\"Question: {o_query}\\nAnswer: {o_answer}\"\n",
    "        g_doc = f\"Question: {g_query}\\nAnswer: {g_answer}\"\n",
    "        \n",
    "        # 可能的召回策略是：\n",
    "        # 知识库构建时，o_query，g_query，o_doc，g_doc 均做向量化。\n",
    "        # 查询时去做QQ召回，QD召回 (认为QA召回一定不如QD召回)\n",
    "        \n",
    "        # 求 origin_Q - generated_Q 的相似性， 判断QQ召回的可行性\n",
    "        qq_labels.append((o_query, g_query)) \n",
    "        \n",
    "        #求 origin_Q - generated_D 的相似性， 判断QD召回的可行性\n",
    "        oqgd_labels.append((o_query, g_doc)) \n",
    "        \n",
    "        #求 origin_Q - origin_D 的相似性， 判断QD召回的可行性\n",
    "        gqod_labels.append((g_query, o_doc))\n",
    "        \n",
    "    qq1_file = open(f\"chatgpt_synthesis/qq1_{output_suffix}.jsonl\", 'w')\n",
    "    for idx, (query_a, query_b) in enumerate(qq_labels):\n",
    "        N = range(len(qq_labels))\n",
    "        m = 20\n",
    "        idx_list = random.sample(N, m)\n",
    "        neg_list = [ qq_labels[i][1] for i in idx_list if i != idx ]\n",
    "        record = json.dumps({ \"query\": query_a, \"pos\": [query_b], \"neg\": neg_list }, ensure_ascii=False)\n",
    "        qq1_file.write(record)\n",
    "        qq1_file.write('\\n')\n",
    "        \n",
    "    qq2_file = open(f\"chatgpt_synthesis/qq2_{output_suffix}.jsonl\", 'w')\n",
    "    for idx, (query_a, query_b) in enumerate(qq_labels):\n",
    "        N = range(len(qq_labels))\n",
    "        m = 20\n",
    "        idx_list = random.sample(N, m)\n",
    "        neg_list = [ qq_labels[i][0] for i in idx_list if i != idx ]\n",
    "        record = json.dumps({ \"query\": query_a, \"pos\": [query_b], \"neg\": neg_list }, ensure_ascii=False)\n",
    "        qq2_file.write(record)\n",
    "        qq2_file.write('\\n')\n",
    "    \n",
    "    oqgd_file = open(f\"chatgpt_synthesis/oqgd_{output_suffix}.jsonl\", 'w')\n",
    "    for idx, (query_o, doc_g) in enumerate(oqgd_labels):\n",
    "        N = range(len(oqgd_labels))\n",
    "        m = 20\n",
    "        idx_list = random.sample(N, m)\n",
    "        neg_list = [ oqgd_labels[i][1] for i in idx_list if i != idx ]\n",
    "        record = json.dumps({ \"query\": \"为这个句子生成表示以用于检索相关文章：\" + query_o, \"pos\": [doc_g], \"neg\": neg_list }, ensure_ascii=False)\n",
    "        oqgd_file.write(record)\n",
    "        oqgd_file.write('\\n')\n",
    "        \n",
    "    gqod_file = open(f\"chatgpt_synthesis/gqod_{output_suffix}.jsonl\", 'w')\n",
    "    for idx, (query_g, doc_o) in enumerate(gqod_labels):\n",
    "        N = range(len(gqod_labels))\n",
    "        m = 20\n",
    "        idx_list = random.sample(N, m)\n",
    "        neg_list = [ gqod_labels[i][1] for i in idx_list if i != idx ]\n",
    "        record = json.dumps({ \"query\": \"为这个句子生成表示以用于检索相关文章：\" + query_g, \"pos\": [doc_o], \"neg\": neg_list }, ensure_ascii=False)\n",
    "        gqod_file.write(record)\n",
    "        gqod_file.write('\\n')\n",
    "    \n",
    "\n",
    "def generate_FAQ(data_arr, sep='\\n=====\\n', faq_name='enhanced_faq'):\n",
    "    def generate_item(data_arr):\n",
    "        for line in data_arr:\n",
    "            json_obj = json.loads(line)\n",
    "\n",
    "            o_query = json_obj['origin_question'].strip()\n",
    "            o_answer = json_obj['origin_answer']\n",
    "            g_query = json_obj['generate_question']\n",
    "            g_answer = json_obj['generate_answer']\n",
    "\n",
    "            faq_template = \"Question: {}\\nAnswer: {}\"\n",
    "\n",
    "            if len(o_query) > 5:\n",
    "                faq1 = faq_template.format(o_query, o_answer)\n",
    "                yield faq1\n",
    "            faq2 = faq_template.format(g_query, o_answer)\n",
    "            yield faq2\n",
    "            faq3 = faq_template.format(g_query, g_answer)\n",
    "            yield faq2\n",
    "            \n",
    "    with open(f\"{faq_name}.faq\", 'w') as outfile:\n",
    "        outfile.write(sep.join(generate_item(data_arr)))\n",
    "            \n",
    "    \n",
    "test_data= None\n",
    "train_data = None\n",
    "with open(FAQ_FILE, 'r') as file:\n",
    "    data_arr = file.readlines()\n",
    "    data_count = len(data_arr)\n",
    "    train_count = int(data_count * 0.9)\n",
    "    test_count = data_count - train_count\n",
    "    test_data = data_arr[:test_count]\n",
    "    train_data = data_arr[test_count:]\n",
    "    valid_data = data_arr[train_count:]\n",
    "    \n",
    "if not os.path.exists('chatgpt_synthesis'):\n",
    "    os.mkdir('chatgpt_synthesis')\n",
    "generate_train_data(test_data, 'test')\n",
    "generate_train_data(train_data, 'train')\n",
    "generate_train_data(valid_data, 'valid')\n",
    "generate_FAQ(data_arr, faq_name=\"chatgpt_enhanced_faq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96489db5-0989-4d56-bc6f-a37d1a1e09f8",
   "metadata": {},
   "source": [
    "### check train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4d5113-c73d-4ab7-9e95-66f0cc12900d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat ./chatgpt_synthesis/*train.jsonl > chatgpt_synthesis/train_merged.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8e7d4c-53ce-4968-94cd-5dee52b6e190",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l chatgpt_synthesis/train_merged.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71fd37e-7f55-4234-a308-f5bdb140c13e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat ./chatgpt_synthesis/*test.jsonl > chatgpt_synthesis/test_merged.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de63e1ae-a552-4b3c-a58e-1d789f095360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wc -l chatgpt_synthesis/test_merged.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eda27e-2b71-4bb8-92f6-f970ab0c6025",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat ./chatgpt_synthesis/*valid.jsonl > chatgpt_synthesis/valid_merged.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913671f7-88f1-4226-b613-ba57a24d2351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wc -l chatgpt_synthesis/valid_merged.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5eda6f-eddb-4512-ad5a-0381ce97e3bc",
   "metadata": {},
   "source": [
    "### Generate hard negative label (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77927f04-c4fc-42a2-b776-3cb28299e30d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -m FlagEmbedding.baai_general_embedding.finetune.hn_mine \\\n",
    "--model_name_or_path BAAI/bge-large-zh \\\n",
    "--input_file chatgpt_synthesis/train_merged.jsonl \\\n",
    "--output_file chatgpt_synthesis/train_merged_minedHN.jsonl \\\n",
    "--range_for_sampling 2-200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ee71c7-0d93-4984-b315-2e92012eccfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_file = open('random_sample_traindata_hardneg.jsonl', 'w')\n",
    "with open('random_sample_traindata_minedHN.jsonl', 'r') as file:\n",
    "    for line in file.readlines():\n",
    "        json_obj = json.loads(line)\n",
    "        out_file.write(json.dumps(json_obj, ensure_ascii=False))\n",
    "        out_file.write('\\n')\n",
    "        \n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fd44c3-f02c-4cd9-b948-49fb752c98c4",
   "metadata": {},
   "source": [
    "### Finetune with Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d6c851-9daa-4b5f-b309-5a1857427e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --nproc_per_node 4 \\\n",
    "-m FlagEmbedding.baai_general_embedding.finetune.run \\\n",
    "--output_dir ./finetune_bge_large_zh15 \\\n",
    "--model_name_or_path BAAI/bge-large-zh-v1.5 \\\n",
    "--train_data ./chatgpt_synthesis/train_merged.jsonl \\\n",
    "--learning_rate 1e-5 \\\n",
    "--fp16 \\\n",
    "--num_train_epochs 5 \\\n",
    "--per_device_train_batch_size 1 \\\n",
    "--normlized True \\\n",
    "--temperature 0.02 \\\n",
    "--query_max_len 128 \\\n",
    "--passage_max_len 512 \\\n",
    "--train_group_size 9 \\\n",
    "--logging_steps 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdaa050-ecc4-4ecf-819e-e663529e2dce",
   "metadata": {},
   "source": [
    "### Deploy Model endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c8f0e8-10d2-46f1-823d-fff5c079906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1023771-e979-4af8-9c7e-cb386bca0157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_model_prefix = \"LLM-RAG/workshop/finetuned-bge15-large-zh-model\"  # folder where model checkpoint will go\n",
    "model_snapshot_path = \"./finetune_bge_large_zh15\"\n",
    "s3_code_prefix = \"LLM-RAG/workshop/finetuned-bge15-large-zh-code\"\n",
    "print(f\"s3_code_prefix: {s3_code_prefix}\")\n",
    "print(f\"model_snapshot_path: {model_snapshot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c9406-be14-4a80-b625-6c339aaf611f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive {model_snapshot_path} s3://{bucket}/{s3_model_prefix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6094f6-f99d-414b-b7b3-14701b05d926",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_image_uri = (\n",
    "    f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.23.0-deepspeed0.9.5-cu118\"\n",
    ")\n",
    "\n",
    "#中国区需要替换为下面的image_uri\n",
    "# inference_image_uri = (\n",
    "#     f\"727897471807.dkr.ecr.{region}.amazonaws.com.cn/djl-inference:0.21.0-deepspeed0.8.3-cu117\"\n",
    "# )\n",
    "\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d9ef4d-26a0-47c3-95f9-81887b7ef675",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p finetuned-bge15-large-zh-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02719fb0-cc87-4d5e-9c11-c3552ffafdd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile finetuned-bge15-large-zh-code/model.py\n",
    "from djl_python import Input, Output\n",
    "import torch\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from FlagEmbedding import FlagModel\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'--device={device}')\n",
    "\n",
    "def load_model(properties):\n",
    "    tensor_parallel = properties[\"tensor_parallel_degree\"]\n",
    "    model_location = properties['model_dir']\n",
    "    if \"model_id\" in properties:\n",
    "        model_location = properties['model_id']\n",
    "    logging.info(f\"Loading model in {model_location}\")\n",
    "\n",
    "    model =  FlagModel(model_location)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = None\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    global model\n",
    "    if not model:\n",
    "        model = load_model(inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        return None\n",
    "    data = inputs.get_as_json()\n",
    "    \n",
    "    input_sentences = None\n",
    "    inputs = data[\"inputs\"]\n",
    "    if isinstance(inputs, list):\n",
    "        input_sentences = inputs\n",
    "    else:\n",
    "        input_sentences =  [inputs]\n",
    "        \n",
    "    is_query = data[\"is_query\"]\n",
    "    instruction = data[\"instruction\"]\n",
    "    logging.info(f\"inputs: {input_sentences}\")\n",
    "    logging.info(f\"is_query: {is_query}\")\n",
    "    logging.info(f\"instruction: {instruction}\")\n",
    "    \n",
    "    if is_query and instruction:\n",
    "        input_sentences = [ instruction + sent for sent in input_sentences ]\n",
    "        \n",
    "    sentence_embeddings =  model.encode(input_sentences)\n",
    "        \n",
    "    result = {\"sentence_embeddings\": sentence_embeddings}\n",
    "    return Output().add_as_json(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e13400-5f2a-4097-bd6d-9719d0a0702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"option.s3url ==> s3://{bucket}/{s3_model_prefix}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e1306d-3c59-404c-8a46-52d53aa1f7ac",
   "metadata": {},
   "source": [
    "#### 设置 serving.properties，requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b23a17d-9de1-4a32-a7d3-139eb64098df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('finetuned-bge15-large-zh-code/serving.properties', 'w') as file:\n",
    "    prop = f\"\"\"engine=Python\n",
    "option.tensor_parallel_degree=1\n",
    "option.s3url = s3://{bucket}/{s3_model_prefix}/\"\"\"\n",
    "    file.write(prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c92fc5-f4f4-419c-af32-3b11db3cbef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('finetuned-bge15-large-zh-code/requirements.txt', 'w') as file:\n",
    "    requirements = \"\"\"transformers==4.28.1\\nFlagEmbedding\"\"\"\n",
    "    file.write(requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab64e7-4d3e-4219-a8dc-2ff30a89bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm s2e_model.tar.gz\n",
    "!cd finetuned-bge15-large-zh-code && rm -rf \".ipynb_checkpoints\"\n",
    "!tar czvf s2e_model.tar.gz finetuned-bge15-large-zh-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad5e4f1-350e-4fdc-9647-cc58da41a6f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_artifact = sess.upload_data(\"s2e_model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716241c8-b883-44ea-930e-1931a4d359f3",
   "metadata": {},
   "source": [
    "### 4. 创建模型 & 创建endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517a6ae9-d0af-4835-871a-1389573b23a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "import boto3\n",
    "\n",
    "model_name = name_from_base(\"bge15-finetuned\") #Note: Need to specify model_name\n",
    "print(model_name)\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": s3_code_artifact\n",
    "    },\n",
    "    \n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40203a41-93b7-4c35-a108-50febae4bb4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g4dn.xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            # \"VolumeSizeInGB\" : 400,\n",
    "            # \"ModelDataDownloadTimeoutInSeconds\": 2400,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 15*60,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c669b1a-c2d9-4243-812b-d8f0cef6940f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30380eda-37a9-4064-b66c-49b335700671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc423d4-04c1-4f84-8ea2-856631b81df0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_vector_by_sm_endpoint(questions, sm_client, endpoint_name):\n",
    "    parameters = {\n",
    "    }\n",
    "\n",
    "    response_model = sm_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(\n",
    "            {\n",
    "                \"inputs\": questions,\n",
    "                \"is_query\": True,\n",
    "                \"instruction\" :  \"Represent this sentence for searching relevant passages:\"\n",
    "            }\n",
    "        ),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "    # 中文instruction => 为这个句子生成表示以用于检索相关文章：\n",
    "    json_str = response_model['Body'].read().decode('utf8')\n",
    "    json_obj = json.loads(json_str)\n",
    "    embeddings = json_obj['sentence_embeddings']\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79431143-c5ed-4c8d-8751-c0710e85793b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts1 = [\"what is the default brightness setting on this device?\", \"how are you going\"]\n",
    "\n",
    "emb = get_vector_by_sm_endpoint(prompts1, smr_client, endpoint_name)\n",
    "print(len(emb[0]))\n",
    "print(emb)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g5.12xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
